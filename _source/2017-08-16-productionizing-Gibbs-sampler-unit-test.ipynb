{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Unit test for your Gibbs sampler\"\n",
    "excerpt: \"Making your MCMC code ready for production.\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Part 1]({{ site.baseurl }}{% post_url 2017-08-15-productionizing-Gibbs-sampler-integration-test %}), I discussed how to integration test a fast, no-frill Gibbs sampler. In essence, the integration test checks if the posterior estimates are \"close enough\" to the true values. Unfortunately, such a test can be misleading. Indeed:\n",
    "\n",
    "- We can pass the test with wrong code. This happens when our model uses regularization or informative prior, causing the posterior estimates to be \"close enough\" even though the Gibbs sampler is wrong.\n",
    "- We can fail the test with correct code. If the posterior density has weird geometry, our Gibbs sampler may not converge in time. In this case, the posterior estimates are not \"close enough\" even though the Gibbs sampler is correct.\n",
    "\n",
    "Furthermore, the current implementation lacks maintainability:\n",
    "\n",
    "- The integration test can take hours to derive the posterior samples, discouraging engineers from making incremental improvements to the code.\n",
    "- The environment within the loop is littered with variables, making it difficult to make small changes without downstream effects.\n",
    "\n",
    "To solve these problems, [Grosse and Duvenaud (2014)](https://arxiv.org/abs/1412.5218) demonstrates a unit-testing approach for MCMC code. In essence, a unit test requires our code to have an easy-to-check expected behavior. Such a behavior for the full conditional \\\\(p(\\theta \\| x)\\\\) is the following identity:[^1]\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{p(\\theta_1 | x)}{p(\\theta_2 | x)} &= \\frac{p(\\theta_1) p(x|\\theta_1)}{p(\\theta_2)p(x|\\theta_2)} \\qquad \\forall \\theta_1, \\theta_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where \\\\(\\theta_1, \\theta_2\\\\) are two arbitrary values of \\\\(\\theta\\\\).\n",
    "\n",
    "This identity is perfect for a unit test for three reasons:\n",
    "- The identity must hold *exactly*, so we don't rely on testing \"close enough\" behavior.\n",
    "- The identity must hold for any two arbitrary values of \\\\(\\theta\\\\), so mocking and setting up is minimal.\n",
    "- The right hand side contains only terms that are easy to derive. Specifically, \\\\(p(\\theta)\\\\) is the prior, which is whatever the distribution we choose. \\\\(p(x \\| \\theta)\\\\) is the likelihood, which comes directly from the model with no extra work.\n",
    "\n",
    "To facilitate unit testing, we modularize the Gibbs sampler as follows. First, we have a `State` class that contains the current state of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, theta, sigma2):\n",
    "        self.theta = theta\n",
    "        self.sigma2 = sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we have a `Model` class that describes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, theta_prior, sigma2_prior, rng=None):\n",
    "        self.theta_prior = theta_prior\n",
    "        self.sigma2_prior = sigma2_prior\n",
    "        self.rng = rng\n",
    "\n",
    "    # Full Conditional for theta\n",
    "    def cond_theta(self, state, data):\n",
    "        n = len(data)\n",
    "        mean_prior = self.theta_prior.args[0]\n",
    "        variance_prior = self.theta_prior.args[1] ** 2\n",
    "\n",
    "        variance_post = 1 / (1 / variance_prior + n / state.sigma2)\n",
    "        mean_post = variance_post * (mean_prior / variance_prior + n * data.mean() / state.sigma2)\n",
    "        return scipy.stats.norm(mean_post, np.sqrt(variance_post))\n",
    "    \n",
    "    # Full Conditional for sigma2\n",
    "    def cond_sigma2(self, state, data):\n",
    "        n = len(data)\n",
    "        shape_prior = self.sigma2_prior.kwds['a']\n",
    "        scale_prior = self.sigma2_prior.kwds['scale']\n",
    "\n",
    "        shape_post = shape_prior + n / 2\n",
    "        scale_post = scale_prior + np.sum((data - state.theta) ** 2) / 2\n",
    "        return scipy.stats.invgamma(shape_post, scale=scale_post)\n",
    "    \n",
    "    # A gibbs step iterate through theta, sigma2 and update them\n",
    "    def gibbs_step(self, state, data):\n",
    "        state.theta = self.cond_theta(state, data).rvs(random_state=self.rng)\n",
    "        state.sigma2 = self.cond_sigma2(state, data).rvs(random_state=self.rng)\n",
    "\n",
    "    # Joint density, used as the right hand side in the identity to be used in the unit test\n",
    "    def joint_log_p(self, state, data):\n",
    "        return self.theta_prior.logpdf(state.theta) + \\\n",
    "               self.sigma2_prior.logpdf(state.sigma2) + \\\n",
    "               (scipy.stats.norm(loc=state.theta, scale=np.sqrt(state.sigma2))\n",
    "                .logpdf(data).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then test the identity above as follows. Note the use of `pytest`'s fixture to DRY. We randomize our model, state, and data to be extra sure that our Gibbs sampler is always correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import copy\n",
    "\n",
    "@pytest.fixture\n",
    "def random_model():\n",
    "    theta_prior = scipy.stats.norm(np.random.uniform(), np.random.uniform())\n",
    "    sigma2_prior = scipy.stats.invgamma(a=np.random.uniform(),\n",
    "                                        scale=np.random.uniform())\n",
    "    return Model(theta_prior=theta_prior,\n",
    "                 sigma2_prior=sigma2_prior)\n",
    "\n",
    "@pytest.fixture\n",
    "def random_state():\n",
    "    return State(np.random.uniform(), np.random.uniform())\n",
    "\n",
    "@pytest.fixture\n",
    "def random_data():\n",
    "    return np.random.normal(size=10)\n",
    "\n",
    "def test_cond_theta(random_model, random_state, random_data):\n",
    "    cond = random_model.cond_theta(random_state, random_data)\n",
    "    new_state = copy.deepcopy(random_state)\n",
    "    new_state.theta = np.random.uniform()\n",
    "\n",
    "    assert np.allclose(cond.logpdf(new_state.theta) - cond.logpdf(random_state.theta),\n",
    "                       random_model.joint_log_p(new_state, random_data) - \\\n",
    "                       random_model.joint_log_p(random_state, random_data))\n",
    "\n",
    "def test_cond_sigma2(random_model, random_state, random_data):\n",
    "    cond = random_model.cond_sigma2(random_state, random_data)\n",
    "    new_state = copy.deepcopy(random_state)\n",
    "    new_state.sigma2 = np.random.uniform()\n",
    "\n",
    "    assert np.allclose(cond.logpdf(new_state.sigma2) - cond.logpdf(random_state.sigma2),\n",
    "                       random_model.joint_log_p(new_state, random_data) - \\\n",
    "                       random_model.joint_log_p(random_state, random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `$ pytest` in the terminal from the project root folder will check whether we have implemented the Gibbs sampler correctly. Check out [my Github repo](https://github.com/LaDilettante/productionized-mcmc) to see how I arrange this code with to be tested with `pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the no-frill Gibbs sampler against the modular Gibbs sampler\n",
    "\n",
    "The modular Gibbs sampler is very legible and easy to test. However, due to various overhead with creating classes and objects, it can be slower that the no-frill Gibbs sampler that relies exclusively on `numpy`. Therefore, my preferred approach is \n",
    "\n",
    "1. Optimize for legibility while writing the modular Gibbs\n",
    "2. Optimize for performance while writing the no-frill Gibbs\n",
    "3. Check that the no-frill Gibbs produces the same result as the modular Gibbs (by running both using the same seed)\n",
    "4. Use the no-frill Gibbs in production\n",
    "\n",
    "This way, we get the best of both worlds. Below I show how to carry out step (3), checking the no-frill Gibbs against the modular Gibbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the data from known parameters\n",
    "true_theta = 2\n",
    "true_sigma2 = 3.5\n",
    "y = np.random.normal(true_theta, np.sqrt(true_sigma2), size = 1000)\n",
    "\n",
    "# Specify the prior\n",
    "prior = {'mu_0': 10, 'tau2_0': 10000, 'nu_0': 1, 'sigma2_0': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the modular Gibbs and store its result\n",
    "class Storage(object):\n",
    "    def __init__(self, S=0):\n",
    "        self.states = [None] * S\n",
    "        \n",
    "    def add_state(self, state, i):\n",
    "        self.states[i] = copy.copy(state)\n",
    "        \n",
    "    def __getattr__(self, param):\n",
    "        return np.array([getattr(state, param) for state in self.states])\n",
    "\n",
    "# Specify the Model, noticing the seed\n",
    "my_model = Model(theta_prior=scipy.stats.norm(prior['mu_0'], np.sqrt(prior['tau2_0'])), \n",
    "                 sigma2_prior=scipy.stats.invgamma(a=prior['nu_0'] / 2, \n",
    "                                                   scale=(prior['nu_0'] * prior['sigma2_0']) / 2),\n",
    "                 rng=42)\n",
    "\n",
    "samples_gibbs_modular = Storage(2)\n",
    "my_state = State(theta=y.mean(), sigma2=y.var())\n",
    "samples_gibbs_modular.add_state(my_state, 0)\n",
    "for i in range(1, 2):\n",
    "    my_model.gibbs_step(my_state, y)\n",
    "    samples_gibbs_modular.add_state(my_state, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the no-frill Gibbs and store its result\n",
    "def gibbs_simple(S, y, prior, rng=None):\n",
    "    \"\"\"\n",
    "    A no-frill Gibbs sampler\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    S : int, the number of samples\n",
    "    y : data vector\n",
    "    prior : dict, prior parameters\n",
    "    rng : int, random seed\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    ybar = y.mean()\n",
    "\n",
    "    # Initialize storage\n",
    "    theta_samples = np.empty(S)\n",
    "    sigma2_samples = np.empty(S)\n",
    "\n",
    "    # Starting value as the sample variance and mean\n",
    "    sigma2_samples[0] = y.var()\n",
    "    theta_samples[0] = ybar\n",
    "\n",
    "    # Big loop\n",
    "    for s in range(1, S):\n",
    "        # Update theta\n",
    "        tau2_n = 1 / (1 / prior['tau2_0'] + n / sigma2_samples[s - 1])\n",
    "        mu_n = tau2_n * (prior['mu_0'] / prior['tau2_0'] + n * ybar / sigma2_samples[s - 1])\n",
    "        theta_samples[s] = scipy.stats.norm(mu_n, np.sqrt(tau2_n)).rvs(random_state=rng)\n",
    "\n",
    "        # Update sigma2\n",
    "        nu_n = prior['nu_0'] + n\n",
    "        nu_sigma2_n = prior['nu_0'] * prior['sigma2_0'] + sum((y - theta_samples[s]) ** 2)\n",
    "        sigma2_samples[s] = scipy.stats.invgamma(nu_n / 2, scale=nu_sigma2_n / 2).rvs(random_state=rng)\n",
    "        \n",
    "    return {'theta': theta_samples, 'sigma2': sigma2_samples}\n",
    "\n",
    "# Notice that we set the same seed here as above\n",
    "samples_gibbs_simple = gibbs_simple(2, y, prior, rng=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(samples_gibbs_simple['theta'], samples_gibbs_modular.theta)\n",
    "assert np.allclose(samples_gibbs_simple['sigma2'], samples_gibbs_modular.sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real production environment, all these tests should be put in an automated test framework. (See [my Github repo](https://github.com/LaDilettante/productionized-mcmc) as an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postcript: Some tips for legible MCMC code\n",
    "\n",
    "As mentioned above, while writing the modular Gibbs sampler, I optimize for legibility. Below are some tips on doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use Python `@property` to compute functions of your parameters\n",
    "\n",
    "In our MCMC code, we frequently need to compute functions of our parameters. For example, while we parameterize our normal distribution with the variance parameter, `numpy`'s normal distribution may ask for scale, or we may want to extract the precision to monitor. In a simple program it's easy enough to write `sigma = np.sqrt(state.sigma2)` or `tau = 1 / state.sigma2` every time. But in a larger program we want to DRY (\"Don't Repeat Yourself\") and simply write `state.sigma` or `state.tau`. Besides all [the usual benefits of DRY](https://softwareengineering.stackexchange.com/questions/103233/why-is-dry-important), this also lets our code reads more like its statistical intent.\n",
    "\n",
    "To create an instance variable (i.e. `sigma`) that is a function of another instance variable (i.e. `sigma2`), we take advantage of Python's `@property` decorator as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, sigma2):\n",
    "        self.sigma2 = sigma2\n",
    "        \n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return np.sqrt(self.sigma2)\n",
    "    \n",
    "    @property\n",
    "    def tau(self):\n",
    "        return 1 / self.sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the value of `sigma2` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma2: 0.25\n"
     ]
    }
   ],
   "source": [
    "my_state = State(sigma2 = 0.5 ** 2)\n",
    "print(\"sigma2: {}\".format(my_state.sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and `sigma` and `tau` will be automatically available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma:  0.5\n",
      "tau:    4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"sigma:  {}\".format(my_state.sigma))\n",
    "print(\"tau:    {}\".format(my_state.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrap `scipy.stats` distributions into classes for flexibility in parameterization\n",
    "\n",
    "A lot of the times `scipy.stats` distributions don't have the parameterization that I'd like to use. For example, in most Bayesian books, the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) uses the shape/rate parameterization, while `scipy.stats` uses the shape/scale parameterization.\n",
    "\n",
    "In these cases, I like to wrap the `scipy.stats` distributions in a class so that I can pick my own parameterization and naming convention. For example, I wrap the Gamma distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GammaDist(object):\n",
    "    def __init__(self, shape, rate, rng=None):\n",
    "        self.shape = shape\n",
    "        self.rate = rate\n",
    "        self.rng = rng\n",
    "        \n",
    "    def rvs(self):\n",
    "        return scipy.stats.gamma(a=self.shape, scale=1/self.rate).rvs(random_state=self.rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, in my code I can draw a new sample as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91282599799158526"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gamma = GammaDist(1, 2)\n",
    "my_gamma.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, I can also easily refer to the Gamma hyperparameters as `my_gamma.shape` and `my_gamma.rate`, which is more legible than `scipy_gamma.kwds['a']` and `1 / scipy_gamma.kwds['scale']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: We obtain this identity by multiplying both the numerator and the denominator with \\\\( p(x) \\\\)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
